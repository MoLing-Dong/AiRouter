import asyncio
from fastapi import APIRouter, HTTPException, Depends
from fastapi.responses import StreamingResponse
from typing import Optional, List, Dict, Any
from pydantic import BaseModel
import json

from app.core.adapters import ChatRequest, Message, MessageRole
from app.services.load_balancing.router import router
from app.utils.simple_auth import require_api_key
from app.utils.logging_config import get_chat_logger

# Get logger
logger = get_chat_logger()

# Global lock for configuration reloading
_config_reload_lock = asyncio.Lock()

anthropic_router = APIRouter(tags=["Anthropic"])


class MessageContent(BaseModel):
    """Anthropic message content"""

    type: str = "text"
    text: str


class AnthropicMessage(BaseModel):
    """Anthropic message format"""

    role: str  # "user" or "assistant"
    content: List[MessageContent]


class AnthropicMessageRequest(BaseModel):
    """Anthropic messages API request"""

    model: str
    messages: List[AnthropicMessage]
    max_tokens: Optional[int] = 4096
    temperature: Optional[float] = 0.7
    top_p: Optional[float] = 1.0
    top_k: Optional[int] = None
    stop_sequences: Optional[List[str]] = None
    stream: Optional[bool] = False


class AnthropicMessageResponse(BaseModel):
    """Anthropic messages API response"""

    id: str
    type: str = "message"
    role: str = "assistant"
    content: List[Dict[str, Any]]
    model: str
    stop_reason: Optional[str] = None
    stop_sequence: Optional[str] = None
    usage: Optional[Dict[str, Any]] = None


class AnthropicMessageStreamChunk(BaseModel):
    """Anthropic streaming response chunk"""

    type: str
    message: Optional[Dict[str, Any]] = None


@anthropic_router.post("/messages")
async def create_message(
    request: AnthropicMessageRequest, api_key: str = Depends(require_api_key)
):
    """Anthropic compatible messages endpoint"""
    try:
        # è¯·æ±‚å¼€å§‹è®¡æ—¶
        import time

        request_start = time.time()
        logger.info(f"ğŸ“¥ æ”¶åˆ°Anthropicæ¶ˆæ¯è¯·æ±‚ - æ¨¡å‹: {request.model}")

        # å¿«é€Ÿè·¯å¾„ï¼šç›´æ¥è·å–é€‚é…å™¨ï¼ˆé¿å…æ•°æ®åº“æŸ¥è¯¢ï¼‰
        from app.services.adapters import adapter_manager

        adapter = adapter_manager.get_best_adapter_fast(
            request.model, skip_version_check=True
        )

        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°é€‚é…å™¨ï¼Œå°è¯•é‡æ–°åŠ è½½é…ç½®ï¼ˆä»…åœ¨å¿…è¦æ—¶ï¼‰
        if not adapter:
            logger.info(f"âš ï¸ æœªæ‰¾åˆ°æ¨¡å‹é€‚é…å™¨ï¼Œå°è¯•é‡æ–°åŠ è½½é…ç½®: {request.model}")

            # Use lock to prevent multiple simultaneous reloads
            async with _config_reload_lock:
                try:
                    # é‡æ–°åŠ è½½é…ç½®
                    adapter_manager.load_models_from_database()

                    # å†æ¬¡å°è¯•è·å–é€‚é…å™¨
                    adapter = adapter_manager.get_best_adapter_fast(
                        request.model, skip_version_check=True
                    )

                    if not adapter:
                        # è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨ç”¨äºé”™è¯¯æ¶ˆæ¯
                        available_models = list(adapter_manager.model_configs.keys())
                        raise HTTPException(
                            status_code=400,
                            detail=f"Model '{request.model}' is not available. Available models: {available_models}",
                        )
                except Exception as reload_error:
                    logger.error(f"é‡æ–°åŠ è½½é…ç½®å¤±è´¥: {reload_error}")
                    available_models = list(adapter_manager.model_configs.keys())
                    raise HTTPException(
                        status_code=400,
                        detail=f"Model '{request.model}' is not available. Available models: {available_models}",
                    )

        # å¿«é€Ÿæ¶ˆæ¯æ ¼å¼è½¬æ¢ï¼ˆä¼˜åŒ–ç‰ˆæœ¬ï¼‰
        messages = [
            Message(
                role=MessageRole(msg.role),
                content=" ".join([content.text for content in msg.content]),
                name=None,
                function_call=None,
            )
            for msg in request.messages
        ]

        # å¿«é€ŸChatRequestæ„å»º
        chat_request = ChatRequest(
            model=request.model,
            messages=messages,
            temperature=request.temperature,
            max_tokens=request.max_tokens,
            top_p=request.top_p,
            frequency_penalty=0.0,  # Anthropic doesn't use these
            presence_penalty=0.0,  # Anthropic doesn't use these
            tools=None,  # Anthropic doesn't use tools in this format
            tool_choice=None,
            stream=request.stream,
        )

        # æ€»é¢„å¤„ç†æ—¶é—´
        total_prep_time = time.time() - request_start
        logger.info(f"âš¡ Anthropicè¯·æ±‚é¢„å¤„ç†å®Œæˆï¼Œæ€»è€—æ—¶: {total_prep_time*1000:.1f}ms")

        if request.stream:
            logger.info(f"ğŸŒŠ å¯åŠ¨Anthropicæµå¼å“åº”: {chat_request.model}")
            # Stream response - ä½¿ç”¨ä¸ Anthropic API å…¼å®¹çš„å“åº”å¤´
            return StreamingResponse(
                stream_anthropic_message(chat_request),
                media_type="text/event-stream",
                headers={
                    "Cache-Control": "no-cache, no-store, must-revalidate",
                    "Connection": "keep-alive",
                    "X-Accel-Buffering": "no",  # ç¦ç”¨ nginx ç¼“å†²
                    "X-Content-Type-Options": "nosniff",
                    "Access-Control-Allow-Origin": "*",
                    "Access-Control-Allow-Headers": "*",
                    "Transfer-Encoding": "chunked",
                    "Content-Encoding": "identity",  # ç¦ç”¨å‹ç¼©
                    "Pragma": "no-cache",  # HTTP/1.0 å…¼å®¹
                },
                background=None,  # ä¸ä½¿ç”¨åå°ä»»åŠ¡
            )
        else:
            # Normal response - ä½¿ç”¨å¿«é€Ÿé€‚é…å™¨æ–¹æ³•è·å¾—æ›´å¥½æ€§èƒ½
            if not adapter:
                raise HTTPException(
                    status_code=503,
                    detail=f"No available adapter for model {chat_request.model}",
                )

            # ç›´æ¥ä½¿ç”¨é€‚é…å™¨è€Œä¸æ˜¯è·¯ç”±å™¨ä»¥è·å¾—æ›´å¥½æ€§èƒ½
            response = await adapter.chat_completion(chat_request)

            # Convert to Anthropic compatible format
            content_text = response.choices[0]["message"]["content"]
            logger.info(
                f"Content text type: {type(content_text)}, value: {content_text}"
            )

            # Ensure content is always a list format for Anthropic
            if isinstance(content_text, str):
                content_list = [{"type": "text", "text": content_text}]
            elif isinstance(content_text, list):
                content_list = content_text
            else:
                content_list = [{"type": "text", "text": str(content_text)}]

            logger.info(f"Content list: {content_list}")

            return AnthropicMessageResponse(
                id=response.id,
                model=response.model,
                content=content_list,
                stop_reason=response.choices[0].get("finish_reason"),
                usage=response.usage,
            )

    except Exception as e:
        import traceback

        error_detail = (
            f"Anthropic message creation failed: {str(e)}\n{traceback.format_exc()}"
        )
        logger.error(f"API error detail: {error_detail}")
        raise HTTPException(
            status_code=500, detail=f"Anthropic message creation failed: {str(e)}"
        )


async def stream_anthropic_message(request: ChatRequest):
    """Stream Anthropic message response"""
    import time

    start_time = time.time()
    logger.info(f"ğŸš€ å¼€å§‹Anthropicæµå¼å“åº”å¤„ç† - æ¨¡å‹: {request.model}")

    try:
        # å¿«é€Ÿè·å–é€‚é…å™¨ï¼ˆå‡å°‘ä¸­é—´å˜é‡å’Œæ—¶é—´è®¡ç®—ï¼‰
        from app.services.adapters import adapter_manager

        adapter = adapter_manager.get_best_adapter_fast(
            request.model, skip_version_check=True
        )

        if not adapter:
            logger.error(f"âŒ æœªæ‰¾åˆ°æ¨¡å‹é€‚é…å™¨: {request.model}")
            yield f"event: error\ndata: {json.dumps({'type': 'error', 'error': {'type': 'not_found_error', 'message': 'No available adapter for model'}})}\n\n"
            return

        # Call adapter to stream response
        try:
            # Check if adapter supports stream response
            if hasattr(adapter, "stream_chat_completion"):
                # å‘é€æ¶ˆæ¯å¼€å§‹äº‹ä»¶
                message_start_data = {
                    "type": "message_start",
                    "message": {
                        "id": "msg_" + str(int(time.time() * 1000000) % 1000000),
                        "type": "message",
                        "role": "assistant",
                        "content": [],
                        "model": request.model,
                        "stop_reason": None,
                        "stop_sequence": None,
                        "usage": {"input_tokens": 0, "output_tokens": 0},
                    },
                }
                message_start_event = (
                    f"event: message_start\ndata: {json.dumps(message_start_data)}\n\n"
                )
                logger.info(f"ğŸ“¤ å‘é€ message_start äº‹ä»¶")
                yield message_start_event

                # æ€§èƒ½ç›‘æ§æ ‡å¿—
                first_chunk_received = False
                content_block_started = False

                async for chunk in adapter.stream_chat_completion(request):
                    # é¦–ä¸ªchunkæ€§èƒ½è®°å½•
                    if not first_chunk_received:
                        first_chunk_received = True
                        total_delay = time.time() - start_time
                        logger.info(
                            f"âš¡ Anthropicé¦–chunkåˆ°è¾¾ï¼Œæ€»å»¶è¿Ÿ: {total_delay:.3f}s"
                        )

                    # Convert OpenAI-style chunk to Anthropic-style
                    if chunk.startswith("data: "):
                        data_str = chunk[6:].strip()

                        # å¿½ç•¥ [DONE] æ ‡è®°
                        if data_str == "[DONE]":
                            continue

                        try:
                            chunk_data = json.loads(data_str)

                            if "choices" in chunk_data and chunk_data["choices"]:
                                delta = chunk_data["choices"][0].get("delta", {})

                                # å‘é€ content_block_startï¼ˆç¬¬ä¸€æ¬¡æœ‰å†…å®¹æ—¶ï¼‰
                                if (
                                    not content_block_started
                                    and "content" in delta
                                    and delta["content"]
                                ):
                                    content_block_started = True
                                    content_start = {
                                        "type": "content_block_start",
                                        "index": 0,
                                        "content_block": {"type": "text", "text": ""},
                                    }
                                    logger.debug(f"ğŸ“¤ å‘é€ content_block_start äº‹ä»¶")
                                    yield f"event: content_block_start\ndata: {json.dumps(content_start)}\n\n"

                                # å‘é€ content_block_delta
                                if "content" in delta and delta["content"]:
                                    content_delta = {
                                        "type": "content_block_delta",
                                        "index": 0,
                                        "delta": {
                                            "type": "text_delta",
                                            "text": delta["content"],
                                        },
                                    }
                                    # åªè®°å½•ç¬¬ä¸€ä¸ªå’Œæ¯éš”10ä¸ªdeltaï¼ˆé¿å…æ—¥å¿—è¿‡å¤šï¼‰
                                    if not hasattr(
                                        stream_anthropic_message, "delta_count"
                                    ):
                                        stream_anthropic_message.delta_count = 0
                                    stream_anthropic_message.delta_count += 1
                                    yield f"event: content_block_delta\ndata: {json.dumps(content_delta)}\n\n"

                        except json.JSONDecodeError as e:
                            logger.warning(
                                f"âš ï¸ JSONè§£æå¤±è´¥: {data_str[:100]}... é”™è¯¯: {e}"
                            )
                            continue
                        except Exception as e:
                            logger.error(f"âŒ chunkå¤„ç†é”™è¯¯: {e}")
                            continue

                # Send content_block_stop event
                if content_block_started:
                    content_stop = {"type": "content_block_stop", "index": 0}
                    yield f"event: content_block_stop\ndata: {json.dumps(content_stop)}\n\n"

                # Send message_delta event (å¯é€‰ï¼Œç”¨äºä¼ é€’æœ€ç»ˆçŠ¶æ€)
                message_delta = {
                    "type": "message_delta",
                    "delta": {"stop_reason": "end_turn", "stop_sequence": None},
                    "usage": {"output_tokens": 0},  # å¯ä»¥åœ¨è¿™é‡Œä¼ é€’å®é™…çš„ token ä½¿ç”¨é‡
                }
                logger.debug(f"ğŸ“¤ å‘é€ message_delta äº‹ä»¶")
                yield f"event: message_delta\ndata: {json.dumps(message_delta)}\n\n"

                # Send message_stop event
                logger.debug(f"ğŸ“¤ å‘é€ message_stop äº‹ä»¶")
                yield f"event: message_stop\ndata: {json.dumps({'type': 'message_stop'})}\n\n"

                # æµå¼å“åº”å®Œæˆæ—¥å¿—
                total_time = time.time() - start_time
                logger.debug(f"âœ… Anthropicæµå¼å“åº”å®Œæˆ - æ€»è€—æ—¶: {total_time:.3f}ç§’")
            else:
                # If adapter does not support stream, return error
                error_data = {
                    "type": "error",
                    "error": {
                        "type": "streaming_not_supported",
                        "message": "This model does not support streaming",
                    },
                }
                yield f"event: error\ndata: {json.dumps(error_data)}\n\n"
                return

        except Exception as e:
            logger.error(f"Stream response failed: {e}")
            streaming_error_data = {
                "type": "error",
                "error": {
                    "type": "streaming_error",
                    "message": f"Stream response failed: {str(e)}",
                },
            }
            yield f"event: error\ndata: {json.dumps(streaming_error_data)}\n\n"
            return

    except Exception as e:
        internal_error_data = {
            "type": "error",
            "error": {"type": "internal_error", "message": str(e)},
        }
        yield f"event: error\ndata: {json.dumps(internal_error_data)}\n\n"
