import asyncio
import time
from fastapi import APIRouter, HTTPException, Request, Depends
from fastapi.responses import StreamingResponse
from typing import Optional, List, Dict, Any
from pydantic import BaseModel
import json

from app.core.adapters import ChatRequest, Message, MessageRole
from app.services.load_balancing.router import SmartRouter
from app.utils.simple_auth import require_api_key

# Initialize router
router = SmartRouter()
from app.utils.logging_config import get_chat_logger

# Get logger
logger = get_chat_logger()

# Global lock for configuration reloading
_config_reload_lock = asyncio.Lock()

chat_router = APIRouter(tags=["Chat"])


class ChatCompletionRequest(BaseModel):
    """OpenAI compatible chat completion request"""

    model: str
    messages: List[Dict[str, Any]]
    temperature: Optional[float] = 0.7
    max_tokens: Optional[int] = None
    top_p: Optional[float] = 1.0
    frequency_penalty: Optional[float] = 0.0
    presence_penalty: Optional[float] = 0.0
    tools: Optional[List[Dict[str, Any]]] = None
    tool_choice: Optional[str] = None
    stream: Optional[bool] = False
    n: Optional[int] = 1
    stop: Optional[List[str]] = None
    logit_bias: Optional[Dict[str, float]] = None
    user: Optional[str] = None
    thinking: Optional[Dict[str, Any]] = None


class ChatCompletionResponse(BaseModel):
    """OpenAI compatible chat completion response"""

    id: str
    object: str = "chat.completion"
    created: int
    model: str
    choices: List[Dict[str, Any]]
    usage: Optional[Dict[str, Any]] = (
        None  # Change to Any to compatible with different usage formats,
    )
    system_fingerprint: Optional[str] = None


class ChatCompletionChunk(BaseModel):
    """Stream response data chunk"""

    id: str
    object: str = "chat.completion.chunk"
    created: int
    model: str
    choices: List[Dict[str, Any]]


@chat_router.post("/chat/completions")
async def chat_completions(
    request: ChatCompletionRequest, api_key: str = Depends(require_api_key)
):
    """OpenAI compatible chat completion interface"""
    try:
        # è¯·æ±‚å¼€å§‹è®¡æ—¶å’Œé€‚é…å™¨è·å–åˆå¹¶
        request_start = time.time()
        logger.info(f"ğŸ“¥ æ”¶åˆ°èŠå¤©è¯·æ±‚ - æ¨¡å‹: {request.model}")

        # å¿«é€Ÿè·¯å¾„ï¼šç›´æ¥è·å–é€‚é…å™¨ï¼ˆåˆå¹¶å¯¼å…¥å’Œè°ƒç”¨ï¼‰
        from app.services import adapter_manager

        adapter = adapter_manager.get_best_adapter_fast(
            request.model, skip_version_check=True
        )

        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°é€‚é…å™¨ï¼Œå°è¯•é‡æ–°åŠ è½½é…ç½®
        if not adapter:
            logger.info(f"âš ï¸ æœªæ‰¾åˆ°æ¨¡å‹é€‚é…å™¨ï¼Œå°è¯•é‡æ–°åŠ è½½é…ç½®: {request.model}")

            # Use lock to prevent multiple simultaneous reloads
            async with _config_reload_lock:
                try:
                    # é‡æ–°åŠ è½½é…ç½®
                    adapter_manager.load_models_from_database()

                    # å†æ¬¡å°è¯•è·å–é€‚é…å™¨
                    adapter = adapter_manager.get_best_adapter_fast(
                        request.model, skip_version_check=True
                    )

                    if not adapter:
                        # è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨ç”¨äºé”™è¯¯æ¶ˆæ¯
                        available_models = list(adapter_manager.model_configs.keys())
                        raise HTTPException(
                            status_code=400,
                            detail=f"Model '{request.model}' is not available. Available models: {available_models}",
                        )
                except Exception as reload_error:
                    logger.error(f"é‡æ–°åŠ è½½é…ç½®å¤±è´¥: {reload_error}")
                    available_models = list(adapter_manager.model_configs.keys())
                    raise HTTPException(
                        status_code=400,
                        detail=f"Model '{request.model}' is not available. Available models: {available_models}",
                    )

        # å¿«é€Ÿæ¶ˆæ¯æ ¼å¼è½¬æ¢ï¼ˆå‡å°‘ä¸­é—´å˜é‡ï¼‰
        messages = [
            Message(
                role=MessageRole(msg["role"]),
                content=msg["content"],
                name=msg.get("name"),
                function_call=msg.get("function_call"),
            )
            for msg in request.messages
        ]

        # å¿«é€ŸChatRequestæ„å»º
        chat_request = ChatRequest(
            model=request.model,
            messages=messages,
            temperature=request.temperature,
            max_tokens=request.max_tokens,
            top_p=request.top_p,
            frequency_penalty=request.frequency_penalty,
            presence_penalty=request.presence_penalty,
            tools=request.tools,
            tool_choice=request.tool_choice,
            stream=request.stream,
            n=request.n,
            stop=request.stop,
            logit_bias=request.logit_bias,
            user=request.user,
            thinking=request.thinking,
        )

        # æ€»é¢„å¤„ç†æ—¶é—´ï¼ˆå‡å°‘æ—¥å¿—I/Oï¼‰
        total_prep_time = time.time() - request_start
        logger.info(f"âš¡ è¯·æ±‚é¢„å¤„ç†å®Œæˆï¼Œæ€»è€—æ—¶: {total_prep_time*1000:.1f}ms")

        if request.stream:
            logger.info(f"ğŸŒŠ å¯åŠ¨å®æ—¶æµå¼å“åº”ç®¡é“: {chat_request.model}")
            # å®æ—¶æµå¼å“åº”ï¼Œé›¶ç¼“å†²é…ç½®
            return StreamingResponse(
                stream_chat_completion(chat_request),
                media_type="text/event-stream",
                headers={
                    "Cache-Control": "no-cache, no-store, must-revalidate",
                    "Connection": "keep-alive",
                    "X-Accel-Buffering": "no",  # ç¦ç”¨nginxç¼“å†²
                    "X-Content-Type-Options": "nosniff",
                    "Access-Control-Allow-Origin": "*",
                    "Access-Control-Allow-Headers": "*",
                    "Transfer-Encoding": "chunked",
                    "Content-Encoding": "identity",  # ç¦ç”¨å‹ç¼©
                    "Pragma": "no-cache",  # HTTP/1.0å…¼å®¹
                },
                background=None,  # ä¸ä½¿ç”¨åå°ä»»åŠ¡
            )
        else:
            # Normal response - use fast adapter method for better performance
            from app.services import adapter_manager

            adapter = adapter_manager.get_best_adapter_fast(
                chat_request.model, skip_version_check=True
            )

            if not adapter:
                raise HTTPException(
                    status_code=503,
                    detail=f"No available adapter for model {chat_request.model}",
                )

            # Use adapter directly instead of router for better performance
            response = await adapter.chat_completion(chat_request)

            # Convert to OpenAI compatible format
            return ChatCompletionResponse(
                id=response.id,
                created=response.created,
                model=response.model,
                choices=response.choices,
                usage=response.usage,
                system_fingerprint=response.system_fingerprint,
            )

    except Exception as e:
        import traceback

        error_detail = f"Chat completion failed: {str(e)}\n{traceback.format_exc()}"
        logger.error(f"API error detail: {error_detail}")
        raise HTTPException(status_code=500, detail=f"Chat completion failed: {str(e)}")


async def stream_chat_completion(request: ChatRequest):
    """Stream chat completion with real-time forwarding"""
    import asyncio
    import time

    start_time = time.time()
    logger.info(f"ğŸš€ å¼€å§‹æµå¼å“åº”å¤„ç† - æ¨¡å‹: {request.model}")

    try:
        # å¿«é€Ÿè·å–é€‚é…å™¨ï¼ˆå‡å°‘ä¸­é—´å˜é‡å’Œæ—¶é—´è®¡ç®—ï¼‰
        from app.services import adapter_manager

        adapter = adapter_manager.get_best_adapter_fast(
            request.model, skip_version_check=True
        )

        if not adapter:
            logger.error(f"âŒ æœªæ‰¾åˆ°æ¨¡å‹é€‚é…å™¨: {request.model}")
            error_msg = json.dumps({"error": "No available adapter for model"})
            yield f"data: {error_msg}\n\n"
            return

        # Call adapter to stream response
        try:
            # Check if adapter supports stream response
            if hasattr(adapter, "stream_chat_completion"):
                # å¼€å§‹æµå¼å¤„ç†ï¼Œå‡å°‘æ—¥å¿—ä»¥æå‡æ€§èƒ½

                # è®¡æ—¶ï¼šæµå¼ç”Ÿæˆå™¨åˆ›å»º
                generator_start = time.time()

                # å®æ—¶chunkè½¬å‘ç®¡é“ - æ¥æ”¶åˆ°å°±ç«‹å³è½¬å‘
                try:
                    # è·å–é€‚é…å™¨çš„æµå¼å“åº”ç”Ÿæˆå™¨
                    stream_generator = adapter.stream_chat_completion(request)

                    generator_time = time.time() - generator_start
                    logger.info(f"ğŸ”„ ç”Ÿæˆå™¨åˆ›å»ºå®Œæˆ ({generator_time*1000:.1f}ms)")

                    # æ€§èƒ½ç›‘æ§æ ‡å¿—
                    first_chunk_received = False

                    # å®æ—¶è½¬å‘å¾ªç¯ - é›¶å»¶è¿Ÿè½¬å‘
                    async for chunk in stream_generator:
                        # é¦–ä¸ªchunkæ€§èƒ½è®°å½•
                        if not first_chunk_received:
                            first_chunk_received = True
                            total_delay = time.time() - start_time
                            logger.info(f"âš¡ é¦–chunkåˆ°è¾¾ï¼Œæ€»å»¶è¿Ÿ: {total_delay:.3f}s")

                        # ç«‹å³è½¬å‘chunk
                        yield chunk

                except asyncio.CancelledError:
                    logger.info("ğŸ›‘ æµå¼ä¼ è¾“è¢«å®¢æˆ·ç«¯å–æ¶ˆ")
                    raise
                except Exception as stream_error:
                    logger.error(f"âŒ æµå¼ç®¡é“é”™è¯¯: {stream_error}")
                    error_msg = json.dumps(
                        {"error": f"Stream pipeline error: {str(stream_error)}"}
                    )
                    yield f"data: {error_msg}\n\n"
                    return

                total_time = time.time() - start_time
                logger.info(f"âœ… å®æ—¶chunkè½¬å‘å®Œæˆ - æ€»è€—æ—¶: {total_time:.3f}ç§’")

                # å‘é€ç»“æŸæ ‡è®°
                yield "data: [DONE]\n\n"

            else:
                logger.error(f"âŒ é€‚é…å™¨ä¸æ”¯æŒæµå¼å“åº”: {type(adapter).__name__}")
                error_msg = json.dumps(
                    {"error": "This model does not support streaming"}
                )
                yield f"data: {error_msg}\n\n"
                return

        except Exception as e:
            logger.error(f"âŒ æµå¼å“åº”å¤„ç†å¤±è´¥: {str(e)}")
            error_msg = json.dumps({"error": f"Stream response failed: {str(e)}"})
            yield f"data: {error_msg}\n\n"
            return

    except Exception as e:
        logger.error(f"âŒ æµå¼å“åº”åˆå§‹åŒ–å¤±è´¥: {str(e)}")
        error_msg = json.dumps({"error": f"Stream initialization failed: {str(e)}"})
        yield f"data: {error_msg}\n\n"


@chat_router.post("/embeddings")
async def create_embeddings(
    request: Dict[str, Any], api_key: str = Depends(require_api_key)
):
    """Create text embedding"""
    try:
        model = request.get("model", "text-embedding-v1")
        input_text = request.get("input")

        if not input_text:
            raise HTTPException(status_code=400, detail="Missing input parameter")

        # Here we need to implement embedding functionality
        # For now, return example response
        return {
            "object": "list",
            "data": [
                {
                    "object": "embedding",
                    "embedding": [0.1] * 1536,  # Example embedding vector
                    "index": 0,
                }
            ],
            "model": model,
            "usage": {
                "prompt_tokens": len(input_text.split()),
                "total_tokens": len(input_text.split()),
            },
        }
    except Exception as e:
        raise HTTPException(
            status_code=500, detail=f"Create embedding failed: {str(e)}"
        )
